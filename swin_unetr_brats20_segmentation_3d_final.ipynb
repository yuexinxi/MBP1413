{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBP1413 Project\n",
    "Original Script from Project-MONAI/tutorials/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb  \n",
    "Modified by Yuexin Xi for the MBP1413 Group project (03-2024)\n",
    "\n",
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License. \n",
    "\n",
    "# 3D Brain Tumor Segmentation with Swin UNETR (BraTS 21 Challenge)\n",
    "\n",
    "\n",
    "This tutorial uses the [Swin UNETR](https://arxiv.org/pdf/2201.01266.pdf) [1,2] model for the task of brain tumor segmentation using the [BraTS 21](http://braintumorsegmentation.org/) challenge dataset [3,4,5,6]. Swin UNETR ranked among top-performing models in the BraTS 21 validation phase. The architecture of Swin UNETR is demonstrated below\n",
    "\n",
    "![swin_brats](../figures/swin_brats21.png)\n",
    "\n",
    "The following features are included in this tutorial:\n",
    "1. Transforms for dictionary format data.\n",
    "1. Define a new transform according to MONAI transform API.\n",
    "1. Load Nifti image with metadata, load a list of images and stack them.\n",
    "1. Randomly rotate across each axes for data augmentation.\n",
    "1. Randomly adjust the intensity for data augmentation.\n",
    "1. Cache IO and transforms to accelerate training and validation.\n",
    "1. Swin UNETR model, Dice loss function, Mean Dice metric for brain tumor segmentation task.\n",
    "\n",
    "For more information access to pre-trained models and distributed training, please refer to Swin UNETR BraTS 21 official repository:\n",
    "\n",
    "https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/BRATS21\n",
    "\n",
    "## Data Description\n",
    "\n",
    "Modality: MRI\n",
    "Size: 1470 3D volumes (1251 Training + 219 Validation)  \n",
    "Challenge: RSNA-ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge\n",
    "\n",
    "The dataset needs to be downloaded from the official BraTS 21 challenge portal as in the following\n",
    "\n",
    "https://www.synapse.org/#!Synapse:syn27046444/wiki/616992\n",
    "\n",
    "The JSON file containing training and validation sets (internal split) needs to be downloaded from this [link](https://drive.google.com/file/d/1i-BXYe-wZ8R9Vp3GXoajGyqaJ65Jybg1/view?usp=sharing) and placed in the same folder as the dataset. As discussed in the following, this tutorial uses fold 1 for training a Swin UNETR model on the BraTS 21 challenge.\n",
    "\n",
    "### Tumor Characteristics\n",
    "\n",
    "The sub-regions considered for evaluation in the BraTS 21 challenge are the \"enhancing tumor\" (ET), the \"tumor core\" (TC), and the \"whole tumor\" (WT). The ET is described by areas that show hyper-intensity in T1Gd when compared to T1, but also when compared to “healthy” white matter in T1Gd. The TC describes the bulk of the tumor, which is what is typically resected. The TC entails the ET, as well as the necrotic (NCR) parts of the tumor. The appearance of NCR is typically hypo-intense in T1-Gd when compared to T1. The WT describes the complete extent of the disease, as it entails the TC and the peritumoral edematous/invaded tissue (ED), which is typically depicted by the hyper-intense signal in FLAIR [[BraTS 21]](http://braintumorsegmentation.org/).\n",
    "\n",
    "The provided segmentation labels have values of 1 for NCR, 2 for ED, 4 for ET, and 0 for everything else.\n",
    "\n",
    "![image](../figures/fig_brats21.png)\n",
    "\n",
    "Figure from [Baid et al.](https://arxiv.org/pdf/2107.02314v1.pdf) [3]\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "\n",
    "If you find this tutorial helpful, please consider citing [1] and [2]:\n",
    "\n",
    "[1]: Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H. and Xu, D., 2022. Swin UNETR: Swin Transformers for Semantic Segmentation of Brain Tumors in MRI Images. arXiv preprint arXiv:2201.01266.\n",
    "\n",
    "[2]: Tang, Y., Yang, D., Li, W., Roth, H.R., Landman, B., Xu, D., Nath, V. and Hatamizadeh, A., 2022. Self-supervised pre-training of swin transformers for 3d medical image analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 20730-20740).\n",
    "\n",
    "\n",
    "### BraTS Dataset References\n",
    "\n",
    "[3] U.Baid, et al., The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification, arXiv:2107.02314, 2021.\n",
    "\n",
    "[4] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et al. \"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)\", IEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) DOI: 10.1109/TMI.2014.2377694\n",
    "\n",
    "[5] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al., \"Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features\", Nature Scientific Data, 4:170117 (2017) DOI: 10.1038/sdata.2017.117\n",
    "\n",
    "[6] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., \"Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection\", The Cancer Imaging Archive, 2017. DOI: 10.7937/K9/TCIA.2017.KLXWJJ1Q\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_segmentation/swin_unetr_brats21_segmentation_3d.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swin UNETR Model\n",
    "\n",
    "The inputs to [Swin UNETR](https://arxiv.org/pdf/2201.01266.pdf) are 3D multi-modal MRI images with 4 channels.\n",
    "The patch partition block creates non-overlapping patches of the input data and projects them into embedding tokens with a resolution of 128x128x128.\n",
    "The projected tokens are then encoded by using a 3D [Swin Transformer](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.pdf) in which the self-attention is computed within local windows.\n",
    "The interaction between different windows is obtained by using 3D window shifting as illustrated below. \n",
    "\n",
    "![image](../figures/shift_patch.png)\n",
    "\n",
    "The transformer-based encoder is connected to a CNN-decoder via skip connection at multiple resolutions.\n",
    "The segmentation output consists of 3 output channels corresponding to ET, WT, and TC sub-regions and is computed by using a 1x1x1 convolutional layer followed by Sigmoid activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Download dataset and json file\n",
    "\n",
    "- Register and download the official BraTS 21 dataset from the link below and place them into \"TrainingData\" in the dataset folder:\n",
    "\n",
    "  https://www.synapse.org/#!Synapse:syn27046444/wiki/616992\n",
    "  \n",
    "  For example, the address of a single file is as follows:\n",
    "  \n",
    "  \"TrainingData/BraTS2021_01146/BraTS2021_01146_flair.nii.gz\"\n",
    "  \n",
    "\n",
    "- Download the json file from this [link](https://drive.google.com/file/d/1i-BXYe-wZ8R9Vp3GXoajGyqaJ65Jybg1/view?usp=sharing) and placed in the same folder as the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[gdown, nibabel, tqdm, ignite, einops]\"\n",
    "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    "    RandAffined,\n",
    "    GaussianSharpend,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet, UNETR\n",
    "from monai.networks.layers import Norm\n",
    "from monai.losses import DiceLoss\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.apps import download_and_extract\n",
    "from monai.utils import first\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "You can specify a directory with the `MONAI_DATA_DIRECTORY` environment variable.  \n",
    "This allows you to save results and reuse downloads.  \n",
    "If not specified a temporary directory will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '/laudata/ealmasri/MBP1413/'\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)\n",
    "data_dir = '/laudata/ealmasri/MBP1413/'\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup average meter, fold reader, checkpoint saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
    "\n",
    "def save_checkpoint(model, epoch, filename=\"model.pt\", best_acc=0, dir_add=root_dir):\n",
    "    state_dict = model.state_dict()\n",
    "    save_dict = {\"epoch\": epoch, \"best_acc\": best_acc, \"state_dict\": state_dict}\n",
    "    filename = os.path.join(dir_add, filename)\n",
    "    torch.save(save_dict, filename)\n",
    "    print(\"Saving checkpoint\", filename)\n",
    "\n",
    "data_dicts = []\n",
    "train_images = sorted(glob.glob(os.path.join(data_dir, \"MICCAI_BraTS2020_TrainingData\", \"BraTS20_Training_*\", \"*[!g].nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_dir, \"MICCAI_BraTS2020_TrainingData\", \"BraTS20_Training_*\", \"*_seg.nii.gz\")))\n",
    "for idx, label_name in enumerate(train_labels):\n",
    "    idx_1st = idx * 4\n",
    "    idx_2nd = 4 * idx + 1\n",
    "    idx_3rd = 4 * idx + 2\n",
    "    idx_4th = 4 * idx + 3\n",
    "    data_dicts.append({\"image\": [train_images[idx_1st],\n",
    "                                 train_images[idx_2nd],\n",
    "                                 train_images[idx_3rd],\n",
    "                                 train_images[idx_4th]], \n",
    "                        \"label\": train_labels[idx]})\n",
    "\n",
    "# divide dataset into 258 subjects for training, 55 subjects for validation, 54 subjects for testing\n",
    "train_files, val_files = data_dicts[0:258], data_dicts[258:313]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"training data: {train_files}\")\n",
    "print(f\"validation data: {val_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size, data_dir, json_list, fold, roi):\n",
    "    data_dir = data_dir\n",
    "    datalist_json = json_list\n",
    "    train_files, validation_files = datafold_read(datalist=datalist_json, basedir=data_dir, fold=fold)\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.CropForegroundd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                source_key=\"image\",\n",
    "                k_divisible=[roi[0], roi[1], roi[2]],\n",
    "            ),\n",
    "            transforms.RandSpatialCropd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                roi_size=[roi[0], roi[1], roi[2]],\n",
    "                random_size=False,\n",
    "            ),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "            transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "        ]\n",
    "    )\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = data.Dataset(data=train_files, transform=train_transform)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_ds = data.Dataset(data=validation_files, transform=val_transform)\n",
    "    val_loader = data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define transformers for training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi = (128, 128, 128)\n",
    "train_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.CropForegroundd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                source_key=\"image\",\n",
    "                k_divisible=[roi[0], roi[1], roi[2]],\n",
    "            ),\n",
    "            transforms.RandSpatialCropd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                roi_size=[roi[0], roi[1], roi[2]],\n",
    "                random_size=False,\n",
    "            ),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "            transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "        ]\n",
    "    )\n",
    "val_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize validation image and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset using validation files and the specified validation transforms\n",
    "check_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "# Create a DataLoader with a batch size of 1 for visualization purposes\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "\n",
    "# Retrieve the first batch of data from the DataLoader\n",
    "check_data = first(check_loader)\n",
    "\n",
    "# Extract the image and label from the data\n",
    "image, label = (check_data[\"image\"][0][0], check_data[\"label\"][0][0])\n",
    "\n",
    "# Print the shapes of the image and label\n",
    "print(f\"image shape: {image.shape}, label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dataset root directory and hyper-parameters\n",
    "\n",
    "The following hyper-parameters are set for the purpose of this tutorial. However, additional changes, as described below, maybe beneficial. \n",
    "\n",
    "If GPU memory is not sufficient, reduce sw_batch_size to 2 or batch_size to 1. \n",
    "\n",
    "Decrease val_every (validation frequency) to 1 for obtaining more accurate checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sw_batch_size = 4\n",
    "infer_overlap = 0.5\n",
    "max_epochs = 100\n",
    "val_every = 5\n",
    "# Takes about 4 minutes\n",
    "# Create a CacheDataset for training with accelerated caching\n",
    "# CacheRate is set to 1.0, caching the entire dataset for optimal performance\n",
    "# NumWorkers enables multi-threading during caching for faster data loading\n",
    "# apply data augmentation in the compose until probability(?) is hit\n",
    "train_ds = CacheDataset(data=train_files, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "\n",
    "# Alternatively, for experimentation with the regular Dataset, uncomment the line below:\n",
    "# train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "\n",
    "# Use DataLoader to load training data in batches of size 2\n",
    "# Shuffle the data for randomization and apply RandCropByPosNegLabeld for data augmentation\n",
    "# Crop will create 4 samples\n",
    "# The resulting batch size will be 2 x 4, generating 2 batches of 4 images each for network training\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "# Create a CacheDataset for validation with accelerated caching\n",
    "# Similar to training, CacheRate is set to 1.0, caching the entire dataset for optimal performance\n",
    "# NumWorkers enables multi-threading during caching for faster data loading\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_rate=1.0, num_workers=4)\n",
    "\n",
    "# Alternatively, for experimentation with the regular Dataset, uncomment the line below:\n",
    "# val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "# Use DataLoader to load validation data in batches of size 1\n",
    "# NumWorkers enables multi-threading during data loading for faster processing\n",
    "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data shape and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_add = os.path.join(data_dir, \"MICCAI_BraTS2020_TrainingData/BraTS20_Training_002/BraTS20_Training_002_flair.nii.gz\")\n",
    "label_add = os.path.join(data_dir, \"MICCAI_BraTS2020_TrainingData/BraTS20_Training_002/BraTS20_Training_002_seg.nii.gz\")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, 78], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 78])\n",
    "plt.savefig('visualize_before_training.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Swin UNETR model\n",
    "\n",
    "In this scetion, we create Swin UNETR model for the 3-class brain tumor semantic segmentation. We use a feature size of 48. We also use gradient checkpointing (use_checkpoint) for more memory-efficient training. However, use_checkpoint for faster training if enough GPU memory is available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.1,\n",
    "    attn_drop_rate=0.1,\n",
    "    dropout_path_rate=0.1,\n",
    "    use_checkpoint=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "dice_loss = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "post_sigmoid = Activations(sigmoid=True)\n",
    "post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "dice_acc = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=sw_batch_size,\n",
    "    predictor=model,\n",
    "    overlap=infer_overlap,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Train and Validation Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch, loss_func):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    run_loss = AverageMeter()\n",
    "    for idx, batch_data in enumerate(loader):\n",
    "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run_loss.update(loss.item(), n=batch_size)\n",
    "        print(\n",
    "            \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "            \"loss: {:.4f}\".format(run_loss.avg),\n",
    "            \"time {:.2f}s\".format(time.time() - start_time),\n",
    "        )\n",
    "        start_time = time.time()\n",
    "    return run_loss.avg\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    epoch,\n",
    "    acc_func,\n",
    "    model_inferer=None,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    run_acc = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(loader):\n",
    "            data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "            logits = model_inferer(data)\n",
    "            val_labels_list = decollate_batch(target)\n",
    "            val_outputs_list = decollate_batch(logits)\n",
    "            val_output_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs_list]\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred=val_output_convert, y=val_labels_list)\n",
    "            acc, not_nans = acc_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
    "            dice_tc = run_acc.avg[0]\n",
    "            dice_wt = run_acc.avg[1]\n",
    "            dice_et = run_acc.avg[2]\n",
    "            print(\n",
    "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", time {:.2f}s\".format(time.time() - start_time),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    return run_acc.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    acc_func,\n",
    "    scheduler,\n",
    "    model_inferer=None,\n",
    "    start_epoch=0,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    val_acc_max = 0.0\n",
    "    dices_tc = []\n",
    "    dices_wt = []\n",
    "    dices_et = []\n",
    "    dices_avg = []\n",
    "    loss_epochs = []\n",
    "    trains_epoch = []\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        print(time.ctime(), \"Epoch:\", epoch)\n",
    "        epoch_time = time.time()\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch=epoch,\n",
    "            loss_func=loss_func,\n",
    "        )\n",
    "        print(\n",
    "            \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
    "            \"loss: {:.4f}\".format(train_loss),\n",
    "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % val_every == 0 or epoch == 0:\n",
    "            loss_epochs.append(train_loss)\n",
    "            trains_epoch.append(int(epoch))\n",
    "            epoch_time = time.time()\n",
    "            val_acc = val_epoch(\n",
    "                model,\n",
    "                val_loader,\n",
    "                epoch=epoch,\n",
    "                acc_func=acc_func,\n",
    "                model_inferer=model_inferer,\n",
    "                post_sigmoid=post_sigmoid,\n",
    "                post_pred=post_pred,\n",
    "            )\n",
    "            dice_tc = val_acc[0]\n",
    "            dice_wt = val_acc[1]\n",
    "            dice_et = val_acc[2]\n",
    "            val_avg_acc = np.mean(val_acc)\n",
    "            print(\n",
    "                \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", Dice_Avg:\",\n",
    "                val_avg_acc,\n",
    "                \", time {:.2f}s\".format(time.time() - epoch_time),\n",
    "            )\n",
    "            dices_tc.append(dice_tc)\n",
    "            dices_wt.append(dice_wt)\n",
    "            dices_et.append(dice_et)\n",
    "            dices_avg.append(val_avg_acc)\n",
    "            if val_avg_acc > val_acc_max:\n",
    "                print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_avg_acc))\n",
    "                val_acc_max = val_avg_acc\n",
    "                save_checkpoint(\n",
    "                    model,\n",
    "                    epoch,\n",
    "                    best_acc=val_acc_max,\n",
    "                )\n",
    "            scheduler.step()\n",
    "    print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n",
    "    return (\n",
    "        val_acc_max,\n",
    "        dices_tc,\n",
    "        dices_wt,\n",
    "        dices_et,\n",
    "        dices_avg,\n",
    "        loss_epochs,\n",
    "        trains_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "(\n",
    "    val_acc_max,\n",
    "    dices_tc,\n",
    "    dices_wt,\n",
    "    dices_et,\n",
    "    dices_avg,\n",
    "    loss_epochs,\n",
    "    trains_epoch,\n",
    ") = trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=dice_loss,\n",
    "    acc_func=dice_acc,\n",
    "    scheduler=scheduler,\n",
    "    model_inferer=model_inferer,\n",
    "    start_epoch=start_epoch,\n",
    "    post_sigmoid=post_sigmoid,\n",
    "    post_pred=post_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best average dice: {val_acc_max:.4f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss and Dice metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, loss_epochs, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_avg, color=\"green\")\n",
    "plt.show()\n",
    "plt.savefig('loss_dice_metric1.png')\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_tc, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_wt, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_et, color=\"purple\")\n",
    "plt.show()\n",
    "plt.savefig('loss_dice_metric2.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create test set dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_num = \"369\"\n",
    "\n",
    "test_files = data_dicts[313:367]\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_ds = data.Dataset(data=test_files, transform=test_transform)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best saved checkpoint and perform inference \n",
    "\n",
    "We select a single case from the validation set and perform inference to compare the model segmentation output with the corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"model.pt\") )[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_inferer_test = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=1,\n",
    "    predictor=model,\n",
    "    overlap=0.6,\n",
    ")\n",
    "\n",
    "acc_func = dice_acc\n",
    "epoch = 0\n",
    "run_acc = AverageMeter()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch_data in enumerate(test_loader):\n",
    "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        logits = model_inferer_test(data)\n",
    "        test_labels_list = decollate_batch(target)\n",
    "        test_outputs_list = decollate_batch(logits)\n",
    "        test_output_convert = [post_pred(post_sigmoid(test_pred_tensor)) for test_pred_tensor in test_outputs_list]\n",
    "        acc_func.reset()\n",
    "        acc_func(y_pred=test_output_convert, y=test_labels_list)\n",
    "        acc, not_nans = acc_func.aggregate()\n",
    "        run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
    "        dice_tc = run_acc.avg[0]\n",
    "        dice_wt = run_acc.avg[1]\n",
    "        dice_et = run_acc.avg[2]\n",
    "        print(\n",
    "            \"Test {}/{} {}/{}\".format(epoch, max_epochs, idx, len(test_loader)),\n",
    "            \", dice_tc:\",\n",
    "            dice_tc,\n",
    "            \", dice_wt:\",\n",
    "            dice_wt,\n",
    "            \", dice_et:\",\n",
    "            dice_et,\n",
    "        )\n",
    "\n",
    "print(\"Dice_avg:\", np.mean(run_acc.avg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize segmentation output and compare with label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the best saved checkpoint and perform inference \n",
    "\n",
    "We select a single case from the validation set and perform inference to compare the model segmentation output with the corresponding label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"model.pt\"))[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_inferer_test = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=1,\n",
    "    predictor=model,\n",
    "    overlap=0.6,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        image = batch_data[\"image\"].to(device)\n",
    "        prob = torch.sigmoid(model_inferer_test(image))\n",
    "        seg = prob[0].detach().cpu().numpy()\n",
    "        seg = (seg > 0.5).astype(np.int8)\n",
    "        seg_out = np.zeros((seg.shape[1], seg.shape[2], seg.shape[3]))\n",
    "        seg_out[seg[1] == 1] = 2\n",
    "        seg_out[seg[0] == 1] = 1\n",
    "        seg_out[seg[2] == 1] = 4\n",
    "\n",
    "slice_num = 67\n",
    "img_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"MICCAI_BraTS2020_TrainingData/BraTS20_Training_367/BraTS20_Training_367_flair.nii.gz\"\n",
    ")\n",
    "label_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"MICCAI_BraTS2020_TrainingData/BraTS20_Training_367/BraTS20_Training_367_seg.nii.gz\"\n",
    ")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, slice_num], cmap=\"gray\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, slice_num])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"segmentation\")\n",
    "plt.imshow(seg_out[:, :, slice_num])\n",
    "plt.savefig('temp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data loader for uncertainty map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_files = data_dicts[366:367]\n",
    "uncertainty_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "uncertainty_ds = data.Dataset(data=uncertainty_files, transform=uncertainty_transform)\n",
    "\n",
    "uncertainty_loader = data.DataLoader(\n",
    "    uncertainty_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "print(uncertainty_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_dropout(model):\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__.startswith('Dropout'):\n",
    "            m.train()\n",
    "\n",
    "def get_monte_carlo_predictions(data_loader,\n",
    "                                forward_passes,\n",
    "                                model):\n",
    "    # dropout_predictions = np.empty((0, n_samples, 240, 240, 155))\n",
    "    list_preds = []\n",
    "    for i in range(forward_passes):\n",
    "        # predictions = np.empty((0, 240, 240, 155))\n",
    "        model.eval()\n",
    "        enable_dropout(model)\n",
    "        for batch_data in data_loader:\n",
    "            image = batch_data[\"image\"].to(device)\n",
    "            with torch.no_grad():\n",
    "                # check if sigmoid and softmax are needed\n",
    "                prob = torch.sigmoid(model_inferer_test(image))\n",
    "                seg = prob[0].detach().cpu().numpy()\n",
    "                list_preds.append(seg)\n",
    "    stack_pred = np.stack(list_preds, axis=0) # shape: (50, 3, ...)\n",
    "    print(stack_pred.shape)\n",
    "\n",
    "    return stack_pred\n",
    "    \n",
    "stack_pred = get_monte_carlo_predictions(uncertainty_loader, 50, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the probability maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_num = 67\n",
    "plt.imshow(stack_pred[25, 1,:,:,slice_num])\n",
    "np.min(stack_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display uncertainty map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_ch1 = np.var(stack_pred, axis=0) # shape: (50, 1, ...)\n",
    "plt.imshow(uncertainty_ch1[0, :, :, slice_num])\n",
    "plt.colorbar()\n",
    "plt.title(\"uncertainty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_drop_out = np.mean(stack_pred, axis=0) # shape:(50, 3, ...)\n",
    "MC_drop_out_bin = post_pred(MC_drop_out)\n",
    "plt.imshow(MC_drop_out_bin[0, :, :, slice_num])\n",
    "plt.title(\"average prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in uncertainty_loader:\n",
    "        image = batch_data[\"image\"].to(device)\n",
    "        prob = torch.sigmoid(model_inferer_test(image))\n",
    "        seg = prob[0].detach().cpu().numpy()\n",
    "        seg = (seg > 0.5).astype(np.int8)\n",
    "        print(seg.shape)\n",
    "        seg_out = np.zeros((seg.shape[1], seg.shape[2], seg.shape[3]))\n",
    "        #seg_out[seg[1] == 1] = 2\n",
    "        seg_out[seg[0] == 1] = 1\n",
    "        #seg_out[seg[2] == 1] = 4\n",
    "\n",
    "slice_num = 67\n",
    "img_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"MICCAI_BraTS2020_TrainingData/BraTS20_Training_369/BraTS20_Training_369_flair.nii.gz\"\n",
    ")\n",
    "label_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"MICCAI_BraTS2020_TrainingData/BraTS20_Training_369/BraTS20_Training_369_seg.nii.gz\"\n",
    ")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, slice_num], cmap=\"gray\")\n",
    "#plt.subplot(1, 3, 2)\n",
    "#plt.title(\"label\")\n",
    "#plt.imshow(label[:, :, slice_num])\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"segmentation\")\n",
    "plt.imshow(seg_out[:, :, slice_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
